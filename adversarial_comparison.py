import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_absolute_error, accuracy_score


def calculate_constraint_violations_on_failures(nurse_pred, doctor_pred, X_raw_fail):
    """è®¡ç®—å¤±æ•ˆæ ·æœ¬ä¸Šçš„çº¦æŸè¿åç‡"""
    violations = 0
    total = len(nurse_pred)
    
    for i in range(total):
        row = X_raw_fail.iloc[i]
        
        # æ£€æŸ¥èµ„æºçº¦æŸ
        if (nurse_pred[i] > row['s_nurse_max'] or doctor_pred[i] > row['s_doctor_max']):
            violations += 1
            continue
            
        # æ£€æŸ¥æ—¶é—´çº¦æŸ
        from generate_adversarial import compute_integrated_system_metrics
        
        # æ ¹æ®åœºæ™¯è°ƒæ•´è¯„ä¼°æ–¹å¼
        if row['scenario'] == 'non_stationary':
            # ç”¨å³°å€¼åˆ°è¾¾ç‡è¯„ä¼°
            lambda_eval = row['lambda_peak'] if 'lambda_peak' in row else row['lambda'] * 1.5
        elif row['scenario'] == 'high_variance_service':
            # è€ƒè™‘å˜å¼‚æ€§å½±å“
            cv_factor = row.get('cv_nurse', 1.5) * 0.3 + row.get('cv_doctor', 1.5) * 0.3
            lambda_eval = row['lambda'] * (1 + cv_factor)
        elif row['scenario'] == 'correlated_service':
            # è€ƒè™‘ç›¸å…³æ€§å½±å“
            corr_factor = row.get('correlation', 0.5) * 0.4
            lambda_eval = row['lambda'] * (1 + corr_factor)
        else:
            lambda_eval = row['lambda']
        
        metrics = compute_integrated_system_metrics(
            lambda_eval, row['mu_nurse'], nurse_pred[i], row['mu_doctor'], doctor_pred[i]
        )
        
        if metrics is None or metrics['system_total_time'] > row['Tmax']:
            violations += 1
    
    return violations / total if total > 0 else 0

def queue_theory_failure_analysis(hybrid_results, traditional_results, queue_results, X_raw_test, y_test):
    """
    ä¸“é—¨åˆ†ææ’é˜Ÿè®ºå¤±æ•ˆçš„åœºæ™¯
    åªå…³æ³¨æ’é˜Ÿè®ºè¡¨ç°ä¸ä½³çš„æƒ…å†µ
    """
    
    # è¯†åˆ«æ’é˜Ÿè®ºå¤±æ•ˆçš„æ ·æœ¬
    queue_failure_mask = X_raw_test['queue_failure'] == 1.0
    queue_violation_mask = X_raw_test['queue_patient_loss'] == 1.0
    
    # åˆå¹¶å¤±æ•ˆæ ·æœ¬
    problematic_mask = queue_failure_mask | queue_violation_mask
    
    if problematic_mask.sum() == 0:
        print("è­¦å‘Šï¼šæ²¡æœ‰å‘ç°æ’é˜Ÿè®ºå¤±æ•ˆçš„æ ·æœ¬ï¼")
        return None, None
    
    print(f"å‘ç° {problematic_mask.sum()} ä¸ªæ’é˜Ÿè®ºè¡¨ç°ä¸ä½³çš„æ ·æœ¬ ({problematic_mask.mean():.2%})")
    
    # åªåˆ†æè¿™äº›æ ·æœ¬
    failure_analysis = {}
    
    methods = {
        'PAN+DNN Hybrid': hybrid_results,
        'Traditional PAN': traditional_results['Traditional_PAN'],
        'Traditional DNN': traditional_results['Traditional_DNN'],
        'Queue Theory': queue_results
    }
    
    for method_name, results in methods.items():
        # åœ¨å¤±æ•ˆæ ·æœ¬ä¸Šçš„è¡¨ç°
        nurse_pred_fail = results['nurse_pred'][problematic_mask]
        doctor_pred_fail = results['doctor_pred'][problematic_mask]
        nurse_true_fail = y_test[problematic_mask, 0]
        doctor_true_fail = y_test[problematic_mask, 1]
        
        # è®¡ç®—çº¦æŸè¿åç‡
        violation_rate = calculate_constraint_violations_on_failures(
            nurse_pred_fail, doctor_pred_fail, X_raw_test[problematic_mask]
        )
        
        failure_analysis[method_name] = {
            'sample_count': problematic_mask.sum(),
            'nurse_mae': mean_absolute_error(nurse_true_fail, nurse_pred_fail),
            'doctor_mae': mean_absolute_error(doctor_true_fail, doctor_pred_fail),
            'nurse_accuracy': accuracy_score(nurse_true_fail, nurse_pred_fail),
            'doctor_accuracy': accuracy_score(doctor_true_fail, doctor_pred_fail),
            'constraint_violation_rate': violation_rate,
            'avg_mae': (mean_absolute_error(nurse_true_fail, nurse_pred_fail) + 
                       mean_absolute_error(doctor_true_fail, doctor_pred_fail)) / 2
        }
    
    return failure_analysis, problematic_mask

def create_queue_theory_failure_visualization(failure_analysis, problematic_mask, X_raw_test):
    """åˆ›å»ºæ’é˜Ÿè®ºå¤±æ•ˆåœºæ™¯çš„ä¸“é—¨å¯è§†åŒ–"""
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
    
    methods = list(failure_analysis.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
    
    # 1. å¤±æ•ˆåœºæ™¯ä¸‹çš„MAEå¯¹æ¯”
    mae_values = [failure_analysis[method]['avg_mae'] for method in methods]
    bars1 = ax1.bar(methods, mae_values, color=colors, alpha=0.8)
    ax1.set_title('æ’é˜Ÿè®ºå¤±æ•ˆåœºæ™¯ä¸‹çš„é¢„æµ‹è¯¯å·®å¯¹æ¯”', fontweight='bold', size=16)
    ax1.set_ylabel('å¹³å‡ç»å¯¹è¯¯å·® (MAE)')
    ax1.tick_params(axis='x', rotation=45)
    
    # æ ‡æ³¨æ•°å€¼å¹¶çªå‡ºç¥ç»ç½‘ç»œä¼˜åŠ¿
    for i, (bar, value) in enumerate(zip(bars1, mae_values)):
        color = 'red' if methods[i] == 'Queue Theory' else 'green' if 'PAN+DNN' in methods[i] else 'black'
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(mae_values)*0.02,
                f'{value:.2f}', ha='center', va='bottom', fontweight='bold', color=color, size=12)
    
    ax1.grid(True, alpha=0.3)
    
    # 2. çº¦æŸè¿åç‡å¯¹æ¯”
    violation_rates = [failure_analysis[method]['constraint_violation_rate'] for method in methods]
    bars2 = ax2.bar(methods, violation_rates, color=colors, alpha=0.8)
    ax2.set_title('æ’é˜Ÿè®ºå¤±æ•ˆåœºæ™¯ä¸‹çš„çº¦æŸè¿åç‡å¯¹æ¯”', fontweight='bold', size=16)
    ax2.set_ylabel('çº¦æŸè¿åç‡')
    ax2.tick_params(axis='x', rotation=45)
    
    for i, (bar, value) in enumerate(zip(bars2, violation_rates)):
        color = 'red' if methods[i] == 'Queue Theory' else 'green' if 'PAN+DNN' in methods[i] else 'black'
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(violation_rates)*0.02,
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold', color=color, size=12)
    
    ax2.grid(True, alpha=0.3)
    
    # 3. æŒ‰åœºæ™¯åˆ†æå¤±æ•ˆæƒ…å†µ
    failure_scenarios = X_raw_test[problematic_mask]['scenario'].value_counts()
    ax3.pie(failure_scenarios.values, labels=failure_scenarios.index, autopct='%1.1f%%', 
            colors=['#FF9999', '#66B2FF', '#99FF99', '#FFCC99'])
    ax3.set_title('æ’é˜Ÿè®ºå¤±æ•ˆåœºæ™¯åˆ†å¸ƒ', fontweight='bold', size=16)
    
    # 4. ç¥ç»ç½‘ç»œç›¸å¯¹ä¼˜åŠ¿
    queue_mae = failure_analysis['Queue Theory']['avg_mae']
    queue_violation = failure_analysis['Queue Theory']['constraint_violation_rate']
    
    improvements = {}
    for method in methods:
        if method != 'Queue Theory':
            mae_improvement = (queue_mae - failure_analysis[method]['avg_mae']) / queue_mae * 100
            violation_improvement = (queue_violation - failure_analysis[method]['constraint_violation_rate']) / max(queue_violation, 0.001) * 100
            improvements[method] = {
                'mae_improvement': max(0, mae_improvement),
                'violation_improvement': max(0, violation_improvement)
            }
    
    # ç»˜åˆ¶æ”¹è¿›å¹…åº¦
    improvement_methods = list(improvements.keys())
    mae_improvements = [improvements[m]['mae_improvement'] for m in improvement_methods]
    violation_improvements = [improvements[m]['violation_improvement'] for m in improvement_methods]
    
    x = np.arange(len(improvement_methods))
    width = 0.35
    
    bars3 = ax4.bar(x - width/2, mae_improvements, width, label='é¢„æµ‹ç²¾åº¦æ”¹è¿›%', alpha=0.8, color='skyblue')
    bars4 = ax4.bar(x + width/2, violation_improvements, width, label='çº¦æŸæ»¡è¶³æ”¹è¿›%', alpha=0.8, color='lightcoral')
    
    ax4.set_xlabel('æ–¹æ³•')
    ax4.set_ylabel('ç›¸å¯¹æ’é˜Ÿè®ºçš„æ”¹è¿›å¹…åº¦ (%)')
    ax4.set_title('ç¥ç»ç½‘ç»œæ–¹æ³•ç›¸å¯¹æ’é˜Ÿè®ºçš„ä¼˜åŠ¿', fontweight='bold', size=16)
    ax4.set_xticks(x)
    ax4.set_xticklabels(improvement_methods, rotation=45)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars3, mae_improvements):
        if value > 0:
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    for bar, value in zip(bars4, violation_improvements):
        if value > 0:
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('queue_theory_failure_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def print_queue_theory_failure_report(failure_analysis, problematic_mask, X_raw_test):
    """æ‰“å°æ’é˜Ÿè®ºå¤±æ•ˆåˆ†ææŠ¥å‘Š"""
    
    print("\n" + "="*80)
    print("                æ’é˜Ÿè®ºå¤±æ•ˆåœºæ™¯åˆ†ææŠ¥å‘Š")
    print("="*80)
    
    print(f"\nå¤±æ•ˆæ ·æœ¬ç»Ÿè®¡:")
    print(f"æ€»æµ‹è¯•æ ·æœ¬: {len(X_raw_test)}")
    print(f"æ’é˜Ÿè®ºå¤±æ•ˆæ ·æœ¬: {problematic_mask.sum()} ({problematic_mask.mean():.2%})")
    
    # æŒ‰åœºæ™¯ç»Ÿè®¡å¤±æ•ˆæƒ…å†µ
    print(f"\nå„åœºæ™¯å¤±æ•ˆåˆ†å¸ƒ:")
    failure_scenarios = X_raw_test[problematic_mask]['scenario'].value_counts()
    for scenario, count in failure_scenarios.items():
        percentage = count / problematic_mask.sum() * 100
        print(f"  {scenario}: {count} æ ·æœ¬ ({percentage:.1f}%)")
    
    print(f"\nå¤±æ•ˆåœºæ™¯ä¸‹å„æ–¹æ³•è¡¨ç°:")
    print("-" * 60)
    
    # æ’åºï¼šæŒ‰çº¦æŸè¿åç‡æ’åº
    sorted_methods = sorted(failure_analysis.items(), 
                          key=lambda x: x[1]['constraint_violation_rate'])
    
    for i, (method, metrics) in enumerate(sorted_methods, 1):
        print(f"{i}. {method}:")
        print(f"   å¹³å‡é¢„æµ‹è¯¯å·®: {metrics['avg_mae']:.3f}")
        print(f"   çº¦æŸè¿åç‡: {metrics['constraint_violation_rate']:.3f}")
        print(f"   æŠ¤å£«é¢„æµ‹å‡†ç¡®ç‡: {metrics['nurse_accuracy']:.3f}")
        print(f"   åŒ»ç”Ÿé¢„æµ‹å‡†ç¡®ç‡: {metrics['doctor_accuracy']:.3f}")
        print()
    
    # è®¡ç®—ç¥ç»ç½‘ç»œç›¸å¯¹æ’é˜Ÿè®ºçš„ä¼˜åŠ¿
    queue_metrics = failure_analysis['Queue Theory']
    hybrid_metrics = failure_analysis['PAN+DNN Hybrid']
    
    mae_improvement = (queue_metrics['avg_mae'] - hybrid_metrics['avg_mae']) / queue_metrics['avg_mae'] * 100
    violation_improvement = (queue_metrics['constraint_violation_rate'] - hybrid_metrics['constraint_violation_rate']) / max(queue_metrics['constraint_violation_rate'], 0.001) * 100
    
    print("ğŸ¯ æ ¸å¿ƒå‘ç°:")
    print("-" * 60)
    print(f"âœ“ åœ¨æ’é˜Ÿè®ºå¤±æ•ˆçš„ {problematic_mask.sum()} ä¸ªå¤æ‚åœºæ™¯ä¸­:")
    print(f"  â€¢ PAN+DNNæ··åˆæ¨¡å‹é¢„æµ‹ç²¾åº¦æ¯”æ’é˜Ÿè®ºé«˜ {mae_improvement:.1f}%")
    print(f"  â€¢ PAN+DNNæ··åˆæ¨¡å‹çº¦æŸè¿åç‡æ¯”æ’é˜Ÿè®ºä½ {violation_improvement:.1f}%")
    
    # è¯†åˆ«PAN+DNNæœ€å¤§ä¼˜åŠ¿çš„åœºæ™¯
    best_scenarios = []
    for scenario in X_raw_test[problematic_mask]['scenario'].unique():
        scenario_mask = (X_raw_test['scenario'] == scenario) & problematic_mask
        if scenario_mask.sum() > 5:  # è‡³å°‘5ä¸ªæ ·æœ¬
            best_scenarios.append(scenario)
    
    if best_scenarios:
        print(f"\nâœ“ PAN+DNNåœ¨ä»¥ä¸‹å¤æ‚åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºæ’é˜Ÿè®º:")
        for scenario in best_scenarios:
            print(f"  â€¢ {scenario} åœºæ™¯")
    
    print(f"\nâœ“ è¿™è¯æ˜äº†ç¥ç»ç½‘ç»œæ–¹æ³•åœ¨å¤„ç†ä»¥ä¸‹æƒ…å†µæ—¶çš„ä¼˜åŠ¿:")
    print(f"  â€¢ éå¹³ç¨³åˆ°è¾¾è¿‡ç¨‹ï¼ˆæ—¶å˜éœ€æ±‚ï¼‰")
    print(f"  â€¢ é«˜å˜å¼‚æ€§æœåŠ¡æ—¶é—´")
    print(f"  â€¢ æœåŠ¡è¿‡ç¨‹ç›¸å…³æ€§")
    print(f"  â€¢ å…¶ä»–è¿åæ’é˜Ÿè®ºåŸºæœ¬å‡è®¾çš„å¤æ‚åœºæ™¯")
    
    print("\n" + "="*80)

def run_adversarial_comparison_analysis(X_train, X_test, y_train, y_test, X_raw_test,
                                      n_nurse_classes, n_doctor_classes, device, model, test_loader, best_params):
    """è¿è¡Œå¯¹æŠ—æ€§å¯¹æ¯”åˆ†æ"""
    
    # å…ˆè¿è¡Œæ ‡å‡†å¯¹æ¯”åˆ†æ
    from train import run_comparison_analysis_with_penalty
    penalty_losses, hybrid_results, traditional_results, queue_results = run_comparison_analysis_with_penalty(
        X_train, X_test, y_train, y_test, X_raw_test,
        n_nurse_classes, n_doctor_classes, device, model, test_loader, best_params
    )
    
    # ç„¶åè¿›è¡Œæ’é˜Ÿè®ºå¤±æ•ˆåˆ†æ
    print("\n=== å¼€å§‹æ’é˜Ÿè®ºå¤±æ•ˆåœºæ™¯ä¸“é¡¹åˆ†æ ===")
    
    failure_analysis, problematic_mask = queue_theory_failure_analysis(
        hybrid_results, traditional_results, queue_results, X_raw_test, y_test
    )
    
    if failure_analysis:
        # åˆ›å»ºå¤±æ•ˆåœºæ™¯å¯è§†åŒ–
        create_queue_theory_failure_visualization(failure_analysis, problematic_mask, X_raw_test)
        
        # æ‰“å°å¤±æ•ˆåˆ†ææŠ¥å‘Š
        print_queue_theory_failure_report(failure_analysis, problematic_mask, X_raw_test)
    
    return penalty_losses, hybrid_results, traditional_results, queue_results, failure_analysis

